# Work Done for Implementation Plan for Chat Mode with RAG Till Now

1. src/app/chat/page.tsx
Client-side React component that serves as the main entry point for the chat interface. It includes authentication checks to ensure only authenticated users can access the chat functionality, with automatic redirection to the login page for unauthenticated users. The component currently provides a placeholder for the chat interface that will be implemented in subsequent steps.

2. src/app/chat/layout.tsx
Layout component that wraps all chat-related pages and provides consistent structure across the chat interface. This component establishes the container for persistent elements that will appear across all chat-related pages, maintaining a consistent height and overflow handling.

3. src/types/chat.ts
Type definitions file that contains all interfaces and enums related to the chat functionality. It includes: LLMProvider enum for different language model providers; ChatModel interface for model information; ChatSettings interface for configuring response generation parameters; ChatMessage interface for individual messages; ChatState interface for the overall chat state; RetrievedChunk, RetrievedDocument, and RetrievedContext interfaces for representing vector search results that will be used as context for responses.

4. src/contexts/ChatContext.tsx
This file defines a React context for managing global chat state across components. It exports the ChatContext object and a useChatContext hook to access the context. The context includes state for the current chat (messages, selected model, settings), functions for chat management (setModel, updateSettings, addMessage, clearMessages), document selection (setSelectedDocuments, setSelectedProjects), context management (setCurrentContext), and session management (startNewChat, loadChat). It also defines a defaultChatState with initial values for a new chat session.

5. src/providers/ChatProvider.tsx
This file implements the ChatProvider component that provides the chat context to the application. It handles localStorage persistence for both chat state and user settings using the LOCAL_STORAGE_CHAT_KEY and LOCAL_STORAGE_SETTINGS_KEY constants. The provider implements all the functions defined in the context type: setModel for changing the current model, updateSettings for modifying chat settings, addMessage for adding new messages to the chat, clearMessages for clearing the chat history, setSelectedDocuments and setSelectedProjects for managing document selection, startNewChat for beginning a fresh chat (with an option to keep current document selections), and loadChat for restoring a previous chat state.

6. src/config/modelConfig.ts
This file defines the configuration for all available LLM models and their settings. It exports: defaultModels, an array of ChatModel objects containing the three specified models (Llama 3.3 70B, DeepSeek R1 Distill Llama 70B, and Llama 4 Maverick 17B) with their parameters; getDefaultSettings, a function that returns model-specific default settings for temperature, token limits, and other parameters; validateSettings, a function that ensures user settings are valid for a given model; and getModelById, a utility function to retrieve model information by ID. The configuration includes model-specific optimizations such as different temperature settings and context window sizes.

7. src/components/chat/ChatLayout.tsx
This component serves as the main layout structure for the chat interface, providing a responsive and feature-rich UI framework. It includes a collapsible sidebar controlled by the isSidebarOpen state variable, a header showing selected project information, a model selection dropdown, and a "New Chat" button. The header dynamically displays the number of selected projects and documents. The component leverages the useChatContext hook to access chat state and actions, and uses the DocumentSidebar component to display selected documents. It also features a toggle button for the sidebar and a placeholder for the model selection dropdown (to be enhanced in future steps).

8. src/components/chat/DocumentSidebar.tsx
This component implements a collapsible document sidebar showing all selected documents grouped by their projects. It dynamically fetches project names and document details from the API, and integrates with the Key Management Service to decrypt document names for encrypted documents. The component handles both encrypted and unencrypted documents, displaying appropriate icons based on document type (PDF, website, YouTube) and security status. It implements expandable project sections using the expandedProjects state, allowing users to collapse and expand document lists by project. Error handling and loading states are provided, including a graceful fallback when document names can't be decrypted immediately. The sidebar also triggers the password prompt when needed to initialize the key service for decryption.

9. src/components/chat/message/UserMessage.tsx
This component renders user messages with a distinct styling to differentiate them from AI messages. It displays messages in right-aligned blue bubbles with rounded corners, using the message prop for content. The component also includes an optional timestamp display that formats the time in a user-friendly way. The message content supports whitespace preservation through the whitespace-pre-wrap CSS property, ensuring proper display of multi-line messages.

10. src/components/chat/message/AIMessage.tsx
This component handles the display of AI-generated responses, including an avatar icon and a collapsible context section. It features a toggle button to show/hide the source documents used for the response via the showContext state. The component accepts a message string, optional timestamp, context object containing retrieved document chunks, and an isLoading flag to show a loading state. When context is provided and showContext is true, it renders the ContextCards component to display the source documents.

11. src/components/chat/message/MessageList.tsx
This component manages the overall chat message display, rendering multiple UserMessage and AIMessage components based on the provided messages array. It implements auto-scrolling functionality using the bottomRef and useEffect to ensure the most recent messages are always visible. When no messages are present, it displays a welcome message with example queries. The component passes the appropriate context to the last AI message only, maintains the chat flow with proper spacing, and shows a loading indicator when isLoading is true.

12. src/components/chat/ContextCards.tsx
This component renders the retrieved context used for generating AI responses, integrating with the existing search result display components. It converts the RetrievedContext data format to the GroupedSearchResult format expected by the SearchResultCard component. The component handles empty context states, calculates maximum similarity scores for each document, and organizes the context by document with a visual separator. It reuses the existing search result UI for consistency across the application, showing similarity scores and expandable document chunks.

13. src/components/chat/ChatInput.tsx
This component implements a user input area for the chat interface with various interactive features. It includes a resizable textarea that automatically adjusts its height based on content using the textareaRef and useEffect hook. The component provides form handling functionality through the handleSubmit function which processes user messages and clears the input after submission. It also implements keyboard shortcuts through the handleKeyDown handler, allowing users to send messages using Enter while creating new lines with Shift+Enter. The UI features a loading state indicator that shows a spinner when messages are being processed, and the submit button is appropriately disabled based on input content and loading state. The component is fully responsive and visually consistent with the rest of the application.

14. src/components/chat/ModelSelection.tsx
This component provides a dropdown interface for selecting different AI models and adjusting their settings. It uses the chat context to access and update model selection and settings through setModel and updateSettings functions. The component features two main interactive elements: a model selector dropdown opened with isOpen state, and a settings panel controlled by isSettingsOpen state. The settings panel includes sliders for adjusting temperature, maximum tokens, similarity threshold, and maximum chunks, with appropriate min/max values and visual feedback. It also includes a toggle for showing context cards. The component handles outside clicks to close dropdowns using event listeners set up in the useEffect hook and the handleClickOutside function. Visual indicators show the currently selected model, and each setting includes helpful labels showing current values and ranges.

15. src/services/llm/LLMProviderInterface.ts: This file defines an abstract provider interface that all LLM providers must implement. It includes methods for generating completions (both regular and streaming), formatting messages with context, validating configurations, and handling errors. The interface ensures consistency across different providers and simplifies adding new providers in the future. It defines the LLMRequestOptions interface for standardized input and StreamingResponse for handling streaming responses.

16. src/services/llm/providers/GroqProvider.ts: This file implements the Groq provider by extending the LLMProviderInterface. It handles configuration through environment variables, formats messages and context appropriately for Groq's API, and implements completion generation using the AI SDK's streamText function. The provider intelligently inserts context as system messages before the last user message and formats context from multiple sources into a readable form. It also includes error handling and validation to ensure the API key is properly configured.

17. src/app/api/chat/route.ts: This file creates the Next.js API route handler for chat requests. It validates incoming requests, finds the appropriate model configuration, initializes the correct provider based on the model's provider type, and passes the formatted messages, context, and settings to generate a completion. The route handles streaming responses and provides appropriate error handling. It's configured with a 2-minute timeout to accommodate longer responses and supports customizable settings that default to model-specific configurations.

18. src/hooks/useChatInitialization.ts (New)
This hook manages the transition from document selection in the retrieval flow to the chat interface. It provides the initializeChat function to set up a new chat session with selected documents, and handleRetrievalCompletion to process completion of the retrieval flow when targeting chat. The hook leverages both the data selection context (for accessing selected documents and projects) and the chat context (for setting up the chat session). It handles document and project ID extraction, navigation to the chat page, and ensures that documents are properly selected before initialization.

19. src\app\dashboard\page.tsx (Modified)
Added a startRetrievalForChat function to the dashboard page that opens the project selection modal specifically configured for chat. This function sets a 'destination' parameter to 'chat' and customizes the title and description to indicate the chat-specific purpose. Also added an onClick handler to the Chat Mode card in the modes array that calls this function, linking the card UI to the retrieval flow functionality.

20. src\hooks\useRetrievalFlow.ts (Modified)
Added a completeRetrieval function to the existing retrieval flow hook to handle different destinations after document selection is complete. When the destination is 'chat', it redirects to the chat page instead of proceeding to the search interface. This enables the retrieval flow to support multiple use cases (search and chat) with appropriate navigation paths for each.

21. src\components\retrieval\DataSelectionModal.tsx (Modified)
Added a conditional "Continue to Chat" button that appears only when the destination is set to 'chat' and documents have been selected. This button closes the modal and navigates the user to the chat page, where the chat initialization hook will handle setting up the chat with the selected documents.

22. src/components/chat/NewChatModal.tsx
This modal component provides users with options for starting a new chat session. It leverages the useChatInitialization and useRetrievalFlow hooks to offer two paths: continuing with the current document selection or selecting new documents. The handleContinueWithCurrent function initializes a new chat with currently selected documents, while handleSelectNewDocuments triggers the document retrieval flow specifically for chat use. The component includes visual indicators showing whether the current selection option is available (based on the hasSelectedDocuments check) and uses a clean, accessible card-based interface with appropriate hover states and icons. The modal uses the application's standard Modal component for consistent UI.

23. src\hooks\useRetrievalFlow.ts (Modified)
Enhanced the retrieval flow hook by refining the startRetrievalForChat function to optionally clear existing document selections before starting a new selection process. Also improved the completeRetrieval function to ensure proper modal closing before navigation to the chat page when the destination is set to 'chat'. The function now ensures a smooth transition from document selection to chat interface. These modifications support the New Chat Modal workflow by providing clear pathways for both continuing with existing selections and starting fresh selections through the retrieval flow.

24. src/services/contextRetrieval.ts
This service provides functions for retrieving and formatting context for LLM-based chat. The primary function retrieveContext takes a query, project IDs, and document IDs, then performs a vector search operation using the existing search utilities. The function encrypts query embeddings client-side, searches across both encrypted and unencrypted documents, and returns context formatted specifically for LLMs. The formatSearchResultsAsContext function converts standard search results into the RetrievedContext format, grouping and filtering chunks by similarity threshold and limiting the total chunk count. Additional utility functions like convertToRetrievedChunk help transform data between different formats. The service maintains the application's zero-trust security model by handling encryption and decryption client-side.

25. src/hooks/useChatVectorSearch.ts
This React hook provides chat-specific vector search capabilities by building on top of the existing useVectorSearch hook. It adds chat-optimized functionality including similarity threshold controls, chunk count limitation, and context formatting for LLM consumption. The hook exposes the core retrieveContextForQuery function that manages retrieving context for chat queries while handling loading states and errors. It also provides updateOptions for adjusting retrieval parameters like threshold and chunk count, and a reset function to clear state. This specialized hook creates a clean separation between general vector search functionality and chat-specific requirements, making it easier to evolve each independently while sharing core vector search capabilities.

26. src/utils/contextProcessing.ts
This utility file provides functions for processing and optimizing context for language models. It includes estimateTokenCount for approximating token usage, prioritizeChunksBySimilarity for sorting chunks by relevance, formatChunksToString for converting chunks into a formatted string, limitContextToTokenBudget for ensuring context fits within model constraints, and createContextSummary for generating a concise overview of included documents. These functions help optimize context retrieval by balancing between relevance and token efficiency, ensuring the most relevant information is provided to the LLM while staying within token limits.

27. src/utils/contextFormatters.ts
This file contains document-specific formatters that extract and present metadata for different document types. It includes specialized functions like formatPdfMetadata (page numbers, sections), formatWebsiteMetadata (URLs, titles), and formatYoutubeMetadata (timestamps, video IDs), with a formatGenericMetadata fallback for other document types. The formatChunkMetadata function dispatches to the appropriate formatter based on document type, while formatTimestamp converts seconds to a friendly time format. The primary function formatContextForLLM structures the entire context for optimal LLM consumption, prioritizing the most relevant chunks and grouping them by source document.

28. src/components/chat/context/ContextDisplay.tsx
This component visualizes retrieved context used in chat responses, making source information transparent to users. It includes three nested components: the main ContextDisplay that manages overall context visibility, DocumentContextCard that displays document-level information with encryption support, and ChunkContextCard that shows individual passages with metadata and similarity scores. The component integrates with KeyManagementService to decrypt document names when necessary, reuses the SimilarityScore component for consistent visualization, and provides expandable/collapsible functionality at both the context and document levels. It formats metadata using the formatChunkMetadata utility for consistent presentation across document types.

29. src\components\chat\message\AIMessage.tsx (Modified)
Updated the AIMessage component to integrate with the new ContextDisplay component. The component now uses a toggle button to show or hide source information and passes the retrieved context directly to the ContextDisplay component when expanded. This modification maintains the clean message UI while providing access to detailed source information when needed, improving transparency and user understanding of AI responses.

30. src/services/chatSessionManager.ts
This service manages chat history, persistence, and context window limitations. It provides static methods for saving, retrieving, and deleting chat sessions from localStorage through functions like saveChat, getChat, getStoredChats, deleteChat, and clearAllChats. The service includes context management methods like limitContextWindow and limitMessagesByTokens which implement intelligent message pruning to fit within token budgets while preserving essential messages (system messages and recent conversation). The estimateTokenCount method provides token estimation for context window management, and generateChatName creates automatic chat titles from message content. The service enforces conversation history limits by maintaining a maximum number of stored chats (defined by MAX_STORED_CHATS) and implements proper versioning and timestamps for each chat session.

31. src/components/chat/NewChatButton.tsx
This component implements a button for starting new chat sessions with integration to the existing NewChatModal. It renders a button with a plus icon and "New Chat" label that opens the modal when clicked. The component manages its own state for modal visibility through the isModalOpen state variable, and provides handlers for both opening (handleNewChatClick) and closing (handleCloseModal) the modal. It also implements a handleQuickNewChat function that starts a new chat while preserving document selections. The button accepts customization props like variant, size, and className for flexible styling integration across different parts of the application.

32. ChatContext.tsx (Modified)
The ChatContext has been enhanced with chat history management by integrating the ChatSessionManager. The startNewChat function now saves the current chat to localStorage before creating a new one, preserving chat history. The addMessage function has been updated to limit the context window using the ChatSessionManager's limitMessagesByTokens method, ensuring conversations stay within token limits while maintaining conversational coherence. A new loadChat function has been added to retrieve past chat sessions from localStorage, enabling users to continue previous conversations. These modifications enable proper context management for LLM responses while maintaining a seamless user experience with chat history persistence.

33. ChatLayout.tsx (Modified)
The chat layout component has been updated to integrate the new NewChatButton component in the header section, replacing the previous placeholder button. This integration adds fully functional new chat capabilities to the main chat interface, allowing users to easily start fresh conversations with proper document selection options.

34. src/hooks/useModelSwitching.ts
This hook manages the process of switching between different language models during an ongoing chat conversation. It provides the switchModel function which changes models while preserving chat history and intelligently adapting settings for the new model. Settings like temperature and penalty values are preserved when possible, while token limits are adjusted to comply with the new model's constraints. The hook also includes the getCompatibleModels function which filters available models based on whether they can handle the current conversation's estimated token count, preventing users from switching to models with insufficient context windows. Additional features include loading state tracking via isSwitching and error handling with detailed error messages. The hook simplifies the complex logic of model transitions, ensuring a smooth user experience when exploring different AI capabilities.

35. ModelSelection.tsx (Modified)
The ModelSelection component has been enhanced with model switching capabilities by integrating the useModelSwitching hook. It now displays compatibility information for each model, showing which ones can handle the current conversation size with visual indicators for incompatible models. The component implements a loading spinner during model transitions and proper error display when switching fails. The handleModelChange function now uses the advanced switchModel function from the hook, preserving user settings where appropriate while ensuring they remain valid for the new model. These enhancements improve the user experience by providing clear feedback during model changes and preventing selection of inappropriate models for the current conversation context.

36. src/components/chat/SettingsPanel.tsx
This component provides a dedicated interface for adjusting model-specific settings in the chat interface. It uses sliders and toggles to allow users to fine-tune parameters like temperature (controlling response randomness), max tokens (limiting response length), top-P (nucleus sampling), frequency penalty, and presence penalty. The component integrates with the chat context through the useChatContext hook, with the handleSettingChange function applying changes to global chat state. For user convenience, it includes a "Reset to defaults" button via handleResetToDefaults that restores model-specific default settings using the getDefaultSettings function. Each control includes descriptive labels and visual feedback, such as showing "More creative" or "More focused" for temperature values, making complex AI parameters more accessible to users.

37. src/components/chat/ContextControls.tsx
This component manages settings specific to the RAG (Retrieval-Augmented Generation) functionality, controlling how document context is retrieved and displayed. It provides three key controls: a similarity threshold slider (determining how closely chunks must match a query to be included), a max chunks slider (limiting the number of document chunks used for context), and a toggle for showing context cards with responses. The component uses the chat context via useChatContext to access and modify these settings through the updateSettings function. Each control includes descriptive text explaining its purpose, with the similarity threshold ranging from 0.1 to 0.99 and max chunks from 1 to 50. The context cards toggle features an accessible switch control that determines whether source documents are displayed alongside AI responses.

38. src/components/chat/ComprehensiveSettings.tsx
This component creates a tabbed interface combining both the model settings and context controls into a single, organized panel. It uses the useState hook to manage which tab is currently active through the activeTab state variable ('model' or 'context'). The component includes tab navigation buttons that update the active tab when clicked, with visual indicators showing which tab is selected. Based on the active tab, it conditionally renders either the SettingsPanel or ContextControls component, passing an optional onSettingsChange callback that allows the parent component to respond to setting changes (typically by closing the panel). This tabbed approach provides an organized way to access all chat settings without overwhelming the user interface.

39. src\app\api\chat\route.ts (Modified)
This file implements the secure API endpoint for chat functionality. It now includes comprehensive security measures: user authentication verification using Supabase's createRouteHandlerClient, document ownership validation through the validateDocumentAccess function, input sanitization with sanitizeInput, and rate limiting via the custom RateLimiter class. The route handler verifies the user's session, checks if they have permission to access the requested documents, sanitizes all inputs to prevent injection attacks, and applies rate limiting to prevent abuse. It returns appropriate HTTP status codes for different error scenarios (401 for authentication failures, 403 for unauthorized document access, 429 for rate limit exceeded) and includes standard rate limit headers in the response. The handler maintains the original functionality of selecting the appropriate LLM provider and generating completions with added security layers.

40. src/lib/documentValidation.ts
This utility module provides document access validation functionality through the validateDocumentAccess function. It checks whether a user has access to the specified documents by querying both the encrypted documents table and unencrypted v2_documents table in the database. The function takes a Supabase client, user ID, and array of document IDs as parameters, then verifies that all requested document IDs belong to the user. It handles both encrypted and unencrypted document types, combines the results, and ensures that every requested document ID is found in the user's collection. The function includes proper error handling and logging for debugging database query issues.

41. src/lib/sanitization.ts
This utility handles input sanitization to prevent injection attacks and XSS vulnerabilities. The sanitizeInput function recursively processes different types of input data: for strings, it uses DOMPurify to sanitize HTML content; for arrays, it applies sanitization to each element; for objects, it processes each property individually; and for primitive values like numbers and booleans, it returns them unchanged. This comprehensive approach ensures all user inputs are properly sanitized before being processed by the API, while preserving the structure and non-string values of complex objects.

42. src/lib/rateLimiter.ts
This module implements a memory-based rate limiting system through the RateLimiter class. It tracks request counts per IP address with configurable time windows and request limits. The class provides a check method that increments the request counter for the client's IP and determines if they've exceeded their limit. It generates standard rate limit headers (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset) and Retry-After headers for responses. The implementation includes automatic cleanup of expired entries and can extract client IP addresses from various request formats, including those behind proxies. This implementation helps protect the API from abuse while providing clients with clear information about their usage limits.

43. ErrorDisplay.tsx
This component provides a standardized way to display errors in the chat interface. It renders a visually distinct error message with options for retrying the operation or dismissing the error. The component includes a collapsible technical details section that can be toggled with the "Show/Hide technical details" button, helping users and developers diagnose issues. It automatically logs errors to the console for debugging and provides a consistent error display pattern across the application. The component accepts customizable retry and dismiss functions through props, making it reusable in various error scenarios.

44. src/components/chat/LoadingStates.tsx
This component provides multiple loading indicator styles for different states in the chat interface. It supports four types of indicators: 'typing' (animated dots for message generation), 'search' (spinner for document searching), 'initial' (pulsing dots for initial loading), and 'progress' (progress bar for operations with known completion percentage). Each type has appropriate animations and can display optional text. The component also exports CSS animations as both a string constant and a React component for global style inclusion. This approach provides consistent, context-appropriate loading indicators throughout the application, improving user experience during asynchronous operations.

45. globals.css (Modified)
Added CSS animations for the loading indicators, particularly the dot-typing animation used in chat responses. The styles define the animated dots with proper timing, positioning, and scaling effects to create a smooth typing indicator. These global styles ensure loading indicators have consistent appearance across the application.
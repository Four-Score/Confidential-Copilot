# Work Done for Implementation Plan for Chat Mode with RAG Till Now

1. src/app/chat/page.tsx
Client-side React component that serves as the main entry point for the chat interface. It includes authentication checks to ensure only authenticated users can access the chat functionality, with automatic redirection to the login page for unauthenticated users. The component currently provides a placeholder for the chat interface that will be implemented in subsequent steps.

2. src/app/chat/layout.tsx
Layout component that wraps all chat-related pages and provides consistent structure across the chat interface. This component establishes the container for persistent elements that will appear across all chat-related pages, maintaining a consistent height and overflow handling.

3. src/types/chat.ts
Type definitions file that contains all interfaces and enums related to the chat functionality. It includes: LLMProvider enum for different language model providers; ChatModel interface for model information; ChatSettings interface for configuring response generation parameters; ChatMessage interface for individual messages; ChatState interface for the overall chat state; RetrievedChunk, RetrievedDocument, and RetrievedContext interfaces for representing vector search results that will be used as context for responses.

4. src/contexts/ChatContext.tsx
This file defines a React context for managing global chat state across components. It exports the ChatContext object and a useChatContext hook to access the context. The context includes state for the current chat (messages, selected model, settings), functions for chat management (setModel, updateSettings, addMessage, clearMessages), document selection (setSelectedDocuments, setSelectedProjects), context management (setCurrentContext), and session management (startNewChat, loadChat). It also defines a defaultChatState with initial values for a new chat session.

5. src/providers/ChatProvider.tsx
This file implements the ChatProvider component that provides the chat context to the application. It handles localStorage persistence for both chat state and user settings using the LOCAL_STORAGE_CHAT_KEY and LOCAL_STORAGE_SETTINGS_KEY constants. The provider implements all the functions defined in the context type: setModel for changing the current model, updateSettings for modifying chat settings, addMessage for adding new messages to the chat, clearMessages for clearing the chat history, setSelectedDocuments and setSelectedProjects for managing document selection, startNewChat for beginning a fresh chat (with an option to keep current document selections), and loadChat for restoring a previous chat state.

6. src/config/modelConfig.ts
This file defines the configuration for all available LLM models and their settings. It exports: defaultModels, an array of ChatModel objects containing the three specified models (Llama 3.3 70B, DeepSeek R1 Distill Llama 70B, and Llama 4 Maverick 17B) with their parameters; getDefaultSettings, a function that returns model-specific default settings for temperature, token limits, and other parameters; validateSettings, a function that ensures user settings are valid for a given model; and getModelById, a utility function to retrieve model information by ID. The configuration includes model-specific optimizations such as different temperature settings and context window sizes.

7. src/components/chat/ChatLayout.tsx
This component serves as the main layout structure for the chat interface, providing a responsive and feature-rich UI framework. It includes a collapsible sidebar controlled by the isSidebarOpen state variable, a header showing selected project information, a model selection dropdown, and a "New Chat" button. The header dynamically displays the number of selected projects and documents. The component leverages the useChatContext hook to access chat state and actions, and uses the DocumentSidebar component to display selected documents. It also features a toggle button for the sidebar and a placeholder for the model selection dropdown (to be enhanced in future steps).

8. src/components/chat/DocumentSidebar.tsx
This component implements a collapsible document sidebar showing all selected documents grouped by their projects. It dynamically fetches project names and document details from the API, and integrates with the Key Management Service to decrypt document names for encrypted documents. The component handles both encrypted and unencrypted documents, displaying appropriate icons based on document type (PDF, website, YouTube) and security status. It implements expandable project sections using the expandedProjects state, allowing users to collapse and expand document lists by project. Error handling and loading states are provided, including a graceful fallback when document names can't be decrypted immediately. The sidebar also triggers the password prompt when needed to initialize the key service for decryption.

9. src/components/chat/message/UserMessage.tsx
This component renders user messages with a distinct styling to differentiate them from AI messages. It displays messages in right-aligned blue bubbles with rounded corners, using the message prop for content. The component also includes an optional timestamp display that formats the time in a user-friendly way. The message content supports whitespace preservation through the whitespace-pre-wrap CSS property, ensuring proper display of multi-line messages.

10. src/components/chat/message/AIMessage.tsx
This component handles the display of AI-generated responses, including an avatar icon and a collapsible context section. It features a toggle button to show/hide the source documents used for the response via the showContext state. The component accepts a message string, optional timestamp, context object containing retrieved document chunks, and an isLoading flag to show a loading state. When context is provided and showContext is true, it renders the ContextCards component to display the source documents.

11. src/components/chat/message/MessageList.tsx
This component manages the overall chat message display, rendering multiple UserMessage and AIMessage components based on the provided messages array. It implements auto-scrolling functionality using the bottomRef and useEffect to ensure the most recent messages are always visible. When no messages are present, it displays a welcome message with example queries. The component passes the appropriate context to the last AI message only, maintains the chat flow with proper spacing, and shows a loading indicator when isLoading is true.

12. src/components/chat/ContextCards.tsx
This component renders the retrieved context used for generating AI responses, integrating with the existing search result display components. It converts the RetrievedContext data format to the GroupedSearchResult format expected by the SearchResultCard component. The component handles empty context states, calculates maximum similarity scores for each document, and organizes the context by document with a visual separator. It reuses the existing search result UI for consistency across the application, showing similarity scores and expandable document chunks.

13. src/components/chat/ChatInput.tsx
This component implements a user input area for the chat interface with various interactive features. It includes a resizable textarea that automatically adjusts its height based on content using the textareaRef and useEffect hook. The component provides form handling functionality through the handleSubmit function which processes user messages and clears the input after submission. It also implements keyboard shortcuts through the handleKeyDown handler, allowing users to send messages using Enter while creating new lines with Shift+Enter. The UI features a loading state indicator that shows a spinner when messages are being processed, and the submit button is appropriately disabled based on input content and loading state. The component is fully responsive and visually consistent with the rest of the application.

14. src/components/chat/ModelSelection.tsx
This component provides a dropdown interface for selecting different AI models and adjusting their settings. It uses the chat context to access and update model selection and settings through setModel and updateSettings functions. The component features two main interactive elements: a model selector dropdown opened with isOpen state, and a settings panel controlled by isSettingsOpen state. The settings panel includes sliders for adjusting temperature, maximum tokens, similarity threshold, and maximum chunks, with appropriate min/max values and visual feedback. It also includes a toggle for showing context cards. The component handles outside clicks to close dropdowns using event listeners set up in the useEffect hook and the handleClickOutside function. Visual indicators show the currently selected model, and each setting includes helpful labels showing current values and ranges.

15. src/services/llm/LLMProviderInterface.ts: This file defines an abstract provider interface that all LLM providers must implement. It includes methods for generating completions (both regular and streaming), formatting messages with context, validating configurations, and handling errors. The interface ensures consistency across different providers and simplifies adding new providers in the future. It defines the LLMRequestOptions interface for standardized input and StreamingResponse for handling streaming responses.

16. src/services/llm/providers/GroqProvider.ts: This file implements the Groq provider by extending the LLMProviderInterface. It handles configuration through environment variables, formats messages and context appropriately for Groq's API, and implements completion generation using the AI SDK's streamText function. The provider intelligently inserts context as system messages before the last user message and formats context from multiple sources into a readable form. It also includes error handling and validation to ensure the API key is properly configured.

17. src/app/api/chat/route.ts: This file creates the Next.js API route handler for chat requests. It validates incoming requests, finds the appropriate model configuration, initializes the correct provider based on the model's provider type, and passes the formatted messages, context, and settings to generate a completion. The route handles streaming responses and provides appropriate error handling. It's configured with a 2-minute timeout to accommodate longer responses and supports customizable settings that default to model-specific configurations.

18. src/hooks/useChatInitialization.ts (New)
This hook manages the transition from document selection in the retrieval flow to the chat interface. It provides the initializeChat function to set up a new chat session with selected documents, and handleRetrievalCompletion to process completion of the retrieval flow when targeting chat. The hook leverages both the data selection context (for accessing selected documents and projects) and the chat context (for setting up the chat session). It handles document and project ID extraction, navigation to the chat page, and ensures that documents are properly selected before initialization.

19. src\app\dashboard\page.tsx (Modified)
Added a startRetrievalForChat function to the dashboard page that opens the project selection modal specifically configured for chat. This function sets a 'destination' parameter to 'chat' and customizes the title and description to indicate the chat-specific purpose. Also added an onClick handler to the Chat Mode card in the modes array that calls this function, linking the card UI to the retrieval flow functionality.

20. src\hooks\useRetrievalFlow.ts (Modified)
Added a completeRetrieval function to the existing retrieval flow hook to handle different destinations after document selection is complete. When the destination is 'chat', it redirects to the chat page instead of proceeding to the search interface. This enables the retrieval flow to support multiple use cases (search and chat) with appropriate navigation paths for each.

21. src\components\retrieval\DataSelectionModal.tsx (Modified)
Added a conditional "Continue to Chat" button that appears only when the destination is set to 'chat' and documents have been selected. This button closes the modal and navigates the user to the chat page, where the chat initialization hook will handle setting up the chat with the selected documents.

22. src/components/chat/NewChatModal.tsx
This modal component provides users with options for starting a new chat session. It leverages the useChatInitialization and useRetrievalFlow hooks to offer two paths: continuing with the current document selection or selecting new documents. The handleContinueWithCurrent function initializes a new chat with currently selected documents, while handleSelectNewDocuments triggers the document retrieval flow specifically for chat use. The component includes visual indicators showing whether the current selection option is available (based on the hasSelectedDocuments check) and uses a clean, accessible card-based interface with appropriate hover states and icons. The modal uses the application's standard Modal component for consistent UI.

23. src\hooks\useRetrievalFlow.ts (Modified)
Enhanced the retrieval flow hook by refining the startRetrievalForChat function to optionally clear existing document selections before starting a new selection process. Also improved the completeRetrieval function to ensure proper modal closing before navigation to the chat page when the destination is set to 'chat'. The function now ensures a smooth transition from document selection to chat interface. These modifications support the New Chat Modal workflow by providing clear pathways for both continuing with existing selections and starting fresh selections through the retrieval flow.

24. src/services/contextRetrieval.ts
This service provides functions for retrieving and formatting context for LLM-based chat. The primary function retrieveContext takes a query, project IDs, and document IDs, then performs a vector search operation using the existing search utilities. The function encrypts query embeddings client-side, searches across both encrypted and unencrypted documents, and returns context formatted specifically for LLMs. The formatSearchResultsAsContext function converts standard search results into the RetrievedContext format, grouping and filtering chunks by similarity threshold and limiting the total chunk count. Additional utility functions like convertToRetrievedChunk help transform data between different formats. The service maintains the application's zero-trust security model by handling encryption and decryption client-side.

25. src/hooks/useChatVectorSearch.ts
This React hook provides chat-specific vector search capabilities by building on top of the existing useVectorSearch hook. It adds chat-optimized functionality including similarity threshold controls, chunk count limitation, and context formatting for LLM consumption. The hook exposes the core retrieveContextForQuery function that manages retrieving context for chat queries while handling loading states and errors. It also provides updateOptions for adjusting retrieval parameters like threshold and chunk count, and a reset function to clear state. This specialized hook creates a clean separation between general vector search functionality and chat-specific requirements, making it easier to evolve each independently while sharing core vector search capabilities.

26. src/utils/contextProcessing.ts
This utility file provides functions for processing and optimizing context for language models. It includes estimateTokenCount for approximating token usage, prioritizeChunksBySimilarity for sorting chunks by relevance, formatChunksToString for converting chunks into a formatted string, limitContextToTokenBudget for ensuring context fits within model constraints, and createContextSummary for generating a concise overview of included documents. These functions help optimize context retrieval by balancing between relevance and token efficiency, ensuring the most relevant information is provided to the LLM while staying within token limits.

27. src/utils/contextFormatters.ts
This file contains document-specific formatters that extract and present metadata for different document types. It includes specialized functions like formatPdfMetadata (page numbers, sections), formatWebsiteMetadata (URLs, titles), and formatYoutubeMetadata (timestamps, video IDs), with a formatGenericMetadata fallback for other document types. The formatChunkMetadata function dispatches to the appropriate formatter based on document type, while formatTimestamp converts seconds to a friendly time format. The primary function formatContextForLLM structures the entire context for optimal LLM consumption, prioritizing the most relevant chunks and grouping them by source document.

28. src/components/chat/context/ContextDisplay.tsx
This component visualizes retrieved context used in chat responses, making source information transparent to users. It includes three nested components: the main ContextDisplay that manages overall context visibility, DocumentContextCard that displays document-level information with encryption support, and ChunkContextCard that shows individual passages with metadata and similarity scores. The component integrates with KeyManagementService to decrypt document names when necessary, reuses the SimilarityScore component for consistent visualization, and provides expandable/collapsible functionality at both the context and document levels. It formats metadata using the formatChunkMetadata utility for consistent presentation across document types.

29. src\components\chat\message\AIMessage.tsx (Modified)
Updated the AIMessage component to integrate with the new ContextDisplay component. The component now uses a toggle button to show or hide source information and passes the retrieved context directly to the ContextDisplay component when expanded. This modification maintains the clean message UI while providing access to detailed source information when needed, improving transparency and user understanding of AI responses.

30. src/services/chatSessionManager.ts
This service manages chat history, persistence, and context window limitations. It provides static methods for saving, retrieving, and deleting chat sessions from localStorage through functions like saveChat, getChat, getStoredChats, deleteChat, and clearAllChats. The service includes context management methods like limitContextWindow and limitMessagesByTokens which implement intelligent message pruning to fit within token budgets while preserving essential messages (system messages and recent conversation). The estimateTokenCount method provides token estimation for context window management, and generateChatName creates automatic chat titles from message content. The service enforces conversation history limits by maintaining a maximum number of stored chats (defined by MAX_STORED_CHATS) and implements proper versioning and timestamps for each chat session.

31. src/components/chat/NewChatButton.tsx
This component implements a button for starting new chat sessions with integration to the existing NewChatModal. It renders a button with a plus icon and "New Chat" label that opens the modal when clicked. The component manages its own state for modal visibility through the isModalOpen state variable, and provides handlers for both opening (handleNewChatClick) and closing (handleCloseModal) the modal. It also implements a handleQuickNewChat function that starts a new chat while preserving document selections. The button accepts customization props like variant, size, and className for flexible styling integration across different parts of the application.

32. ChatContext.tsx (Modified)
The ChatContext has been enhanced with chat history management by integrating the ChatSessionManager. The startNewChat function now saves the current chat to localStorage before creating a new one, preserving chat history. The addMessage function has been updated to limit the context window using the ChatSessionManager's limitMessagesByTokens method, ensuring conversations stay within token limits while maintaining conversational coherence. A new loadChat function has been added to retrieve past chat sessions from localStorage, enabling users to continue previous conversations. These modifications enable proper context management for LLM responses while maintaining a seamless user experience with chat history persistence.

33. ChatLayout.tsx (Modified)
The chat layout component has been updated to integrate the new NewChatButton component in the header section, replacing the previous placeholder button. This integration adds fully functional new chat capabilities to the main chat interface, allowing users to easily start fresh conversations with proper document selection options.